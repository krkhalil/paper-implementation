{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RATIO 2019 - Benchmarking Workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - Same Side Clasiification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cross_path = 'data/same-side-classification/cross-topic/{}.csv'\n",
    "data_within_path = 'data/same-side-classification/within-topic/{}.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load within-topics and cross-topics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i would like to start off by thanking my opponent for his first debate, and hope that he has many more to come. claiming that something should be maintain simply because it is tradition is a common falacy, known as appeal to tradition. examples of how this is not an adequate reason to preserve something by itself are such issues as slavery, and discrimination of women. simply because that is the way things were, does not mean that is the way thing ought be. i would also like to question my opponent\\'s claim that, \"i believe it is the child who chooses to become gay in a more mature state of mind.\" if a child can simply \"choose\" to be gay, then they can \"choose\" to not be gay. also, if someone is capable of such a choice, then my opponent would be able to simply become gay for 30 minutes, then become straight again, as would i and anyone else reading and voting on this debate. as of now, i have not found the way to choose to be gay. even so, i don\\'t see how choosing or not choosing to be gay should effect if gay marriage is allowed. if the government is going to provide financial benefits (along with other benefits) to people that are married, then what reason does the government have for omitting and discriminating against a classification of people? we already know that the government recognizes gay people as a class, thanks to surveys and the military\\'s dadt.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_traindev_df = pd.read_csv(data_cross_path.format('training'), quotechar='\"',quoting=csv.QUOTE_ALL,encoding='utf-8',escapechar='\\\\',doublequote=False,index_col='id')\n",
    "cross_test_df =  pd.read_csv(data_cross_path.format('test'), index_col='id')\n",
    "\n",
    "within_traindev_df =  pd.read_csv(data_within_path.format('training'),quotechar='\"',quoting=csv.QUOTE_ALL,encoding='utf-8',escapechar='\\\\',doublequote=False,index_col='id')\n",
    "within_test_df =  pd.read_csv(data_within_path.format('test'), index_col='id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a tag for the topics in focus: \"gay marriage\" and \"abortion\"\n",
    "def add_tag(row):\n",
    "    title = row['topic'].lower().strip()\n",
    "    if title.find('abortion') > -1 :\n",
    "        row['tag'] = 'abortion'\n",
    "    elif title.find('gay marriage') > -1 :\n",
    "        row['tag'] = 'gay marriage'\n",
    "    else:\n",
    "        row['tag'] = 'NA'\n",
    "    return row\n",
    "\n",
    "cross_traindev_df = cross_traindev_df.apply(add_tag, axis=1)\n",
    "cross_test_df = cross_test_df.apply(add_tag, axis=1)\n",
    "\n",
    "within_traindev_df = within_traindev_df.apply(add_tag, axis=1)\n",
    "within_test_df = within_test_df.apply(add_tag, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "within_traindev_df['tag'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#74517\n",
    "within_traindev_df.loc[82134]['argument2']\n",
    "#within_traindev_df.loc[74517]['argument2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "within_traindev_df[(within_traindev_df['tag'] == 'gay marriage') & (within_traindev_df['tag'] == 'gay marriage')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#within_traindev_df[(within_traindev_df['tag'] == 'gay marriage') and (within_traindev_df['is_same_side'] == 'True')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get an overview about each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_overview(df, task = 'same-side', class_name = 'is_same_side'):\n",
    "    # Total instance numbers\n",
    "    total = len(df)\n",
    "    print(\"Task: \", task, '\\n\\n')\n",
    "    print('Total instances: ', total)\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    print('For each topic:')\n",
    "    for tag, tag_df in df.groupby(['tag']):\n",
    "        print(tag, ': ', len(tag_df), ' instances')\n",
    "        \n",
    "        if class_name in df.columns:\n",
    "            for is_same_side, side_df in tag_df.groupby([class_name]):\n",
    "                print('\\t\\t',is_same_side, ': ', len(side_df), ' instances')\n",
    "            \n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    if class_name in df.columns:\n",
    "        print('For each class value:')\n",
    "        for class_value, class_df in df.groupby([class_name]):\n",
    "            print(class_value, ': ', len(class_df), ' instances')\n",
    "\n",
    "        print()\n",
    "        print()\n",
    "\n",
    "    print('Unique argument1:', len(df['argument1'].unique()))\n",
    "    print('Unique argument2:', len(df['argument2'].unique()))\n",
    "    arguments = df['argument1'].values\n",
    "    arguments = np.concatenate([arguments, df['argument2'].values])\n",
    "\n",
    "    print('Unique total arguments:', len(set(list(arguments))))\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    arguments_length_lst = [len(word_tokenize(x)) for x in df['argument1'].values]\n",
    "    arguments_length_lst.extend([len(word_tokenize(x)) for x in df['argument2'].values])\n",
    "    print('Words:')\n",
    "    print('shortest argument:', min(arguments_length_lst), ' words')\n",
    "    print('longest argument:', max(arguments_length_lst), ' words')\n",
    "    print('aargument average length:', np.mean(arguments_length_lst), ' words')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    arguments_sent_length_lst = [len(sent_tokenize(x)) for x in df['argument1'].values]\n",
    "    arguments_sent_length_lst.extend([len(sent_tokenize(x)) for x in df['argument2'].values])\n",
    "    print('Sentences:')\n",
    "    print('shortest argument:', min(arguments_sent_length_lst), ' sentences')\n",
    "    print('longest argument:', max(arguments_sent_length_lst), ' sentences')\n",
    "    print('aargument average length:', np.mean(arguments_sent_length_lst), ' sentences')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "get_overview(cross_traindev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_overview(within_traindev_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model - Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train dev set - 70% 30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "def get_train_test_sets(df):\n",
    "    X = df[['argument1', 'argument2', 'topic']]\n",
    "    y = df[['is_same_side']]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1, shuffle=True)\n",
    "    return X_train, X_test, y_train, y_test \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "        \"\"\"\n",
    "        return WORDNET POS compliance to WORDENT lemmatization (a,n,r,v) \n",
    "        \"\"\"\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            # As default pos in lemmatization is Noun\n",
    "            return wordnet.NOUN\n",
    "\n",
    "def lemmatize_stemming(token, pos_tag):\n",
    "    stemmer = SnowballStemmer(\"english\") #pOrter, M. \"An algorithm for suffix stripping.\"\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(token, pos=pos_tag))\n",
    "\n",
    "def preprocess(text):\n",
    "    lemma = []\n",
    "    for sentence in sent_tokenize(text):\n",
    "        sentence = sentence.replace('\\n', ' ').strip()\n",
    "        tokens = [token for token in word_tokenize(sentence)]\n",
    "        pos_tags = nltk.pos_tag(tokens)\n",
    "        \n",
    "        for idx in range(0,len(tokens)):\n",
    "            token = tokens[idx].lower()\n",
    "            if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "                wordnet_pos = get_wordnet_pos(pos_tags[idx][1])\n",
    "                l_ = lemmatize_stemming(token, wordnet_pos)\n",
    "                lemma.append(l_)\n",
    "    return ' '.join(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma(row):\n",
    "    row['argument1_lemmas'] = preprocess(row['argument1'])\n",
    "    row['argument2_lemmas'] = preprocess(row['argument2'])\n",
    "    return row\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting n grams lemma for argument1 and argument2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "def extract_ngrams(X_train, X_dev, col, idx='id'):\n",
    "    vectorizer = CountVectorizer(min_df=600, max_df=0.7, ngram_range=(3, 3), max_features=5000 )\n",
    "    \n",
    "    vectorizer.fit(X_train[col])\n",
    "    features = vectorizer.transform(X_train[col])\n",
    "    features_dev = vectorizer.transform(X_dev[col])\n",
    "\n",
    "    train_df =pd.DataFrame(\n",
    "        features.todense(),\n",
    "        columns=vectorizer.get_feature_names()\n",
    "    )\n",
    "    train_df = train_df.add_prefix(col)\n",
    "\n",
    "    \n",
    "    aid_df = X_train[[idx]]\n",
    "\n",
    "    train_df = train_df.merge(aid_df, left_index =True, right_index=True, suffixes=(False, False), how='inner')\n",
    "    train_df.set_index(idx, inplace=True)    \n",
    "    \n",
    "    dev_df =pd.DataFrame(\n",
    "        features_dev.todense(),\n",
    "        columns=vectorizer.get_feature_names()\n",
    "    )\n",
    "    dev_df = dev_df.add_prefix(col)\n",
    "\n",
    "    \n",
    "    aid_dev_df = X_dev[[idx]]\n",
    "\n",
    "    dev_df = dev_df.merge(aid_dev_df, left_index =True, right_index=True, suffixes=(False, False), how='inner')\n",
    "    dev_df.set_index(idx, inplace=True)\n",
    "    return train_df, dev_df\n",
    "\n",
    "def extract_n_grams_features(X_train, X_dev, columns, idx='id'): \n",
    "\n",
    "    X_train = X_train.reset_index()\n",
    "    result_train_df =  X_train[[idx]]\n",
    "    result_train_df.set_index(idx, inplace=True)\n",
    "    \n",
    "    \n",
    "    X_dev = X_dev.reset_index()\n",
    "    result_dev_df =  X_dev[[idx]]\n",
    "    result_dev_df.set_index(idx, inplace=True)\n",
    "    \n",
    "    for col in columns:\n",
    "        result_train_df_, result_dev_df_ = extract_ngrams(X_train, X_dev, col)\n",
    "        result_train_df = result_train_df.join(result_train_df_)\n",
    "        result_dev_df = result_dev_df.join(result_dev_df_)\n",
    "    return result_train_df, result_dev_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC  \n",
    "from sklearn.metrics import classification_report, confusion_matrix , accuracy_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def train_test_svm(X_train, y_train, X_test):\n",
    "    scaler = StandardScaler(copy=True, with_mean=False)\n",
    "    scaler.fit(X_train)\n",
    "    \n",
    "    X_train = scaler.transform(X_train)\n",
    "    \n",
    "    svclassifier = SVC(kernel='linear')  \n",
    "    svclassifier.fit(X_train, y_train)  \n",
    "    \n",
    "    X_test = scaler.transform(X_test)\n",
    "    y_pred = svclassifier.predict(X_test) \n",
    "\n",
    "    return y_pred\n",
    "def report_training_results(y_test, y_pred):\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(y_test, y_pred))  \n",
    "    print()\n",
    "    print('Accuracy: ', round(accuracy_score(y_test, y_pred), 2))  #\n",
    "    print()\n",
    "\n",
    "    print('Report:')\n",
    "    print(classification_report(y_test, y_pred))  \n",
    "    f1_dic = {}\n",
    "    \n",
    "    f1_dic['macro'] = round(f1_score(y_pred=y_pred, y_true=y_test, average='macro'), 2)\n",
    "    f1_dic['micro'] = round(f1_score(y_pred=y_pred, y_true=y_test, average='micro'), 2)\n",
    "    return f1_dic\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross topic - Training and evaluating model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Getting train and dev data\n",
    "X_train, X_dev, y_train, y_dev = get_train_test_sets(cross_traindev_df)\n",
    "\n",
    "\n",
    "# 2. Lemmatizing argument1 and argument2\n",
    "X_train = X_train.apply(get_lemma, axis=1)\n",
    "X_dev = X_dev.apply(get_lemma, axis=1)\n",
    "\n",
    "# 3. Extracting features - 1-3 grams lemma\n",
    "X_train_, X_dev_ = extract_n_grams_features(X_train, X_dev, columns=['argument1_lemmas', 'argument2_lemmas'])\n",
    "\n",
    "# 4 train \n",
    "y_pred = train_test_svm(X_train_, y_train, X_dev_)\n",
    "\n",
    "# 5 Evaluate\n",
    "report_training_results(y_dev, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
